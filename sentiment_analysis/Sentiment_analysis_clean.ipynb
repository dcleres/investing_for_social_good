{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "list_df = []\n",
    "for i in range (10):\n",
    "    list_df.append(pd.read_json(f'../cluster_data/0{i}.json', lines=True))\n",
    "for i in (np.arange(10, 60, 1)):\n",
    "    list_df.append(pd.read_json(f'../cluster_data/{i}.json', lines=True))\n",
    "\n",
    "df = pd.concat(list_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary containing each tweet with the tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hashtags = []\n",
    "dict_tot = dict()\n",
    "for i in range(df.text.index.max()):\n",
    "    dict_tmp = dict()\n",
    "    if (type(df.text[i])==str):\n",
    "        if (len(df.entities[i]['hashtags'])!=0):\n",
    "            dict_tmp['hashtags'] = df.entities[i]['hashtags'][0]['text']\n",
    "            dict_tmp['text'] = df.text[i]\n",
    "        else :\n",
    "            dict_tmp['hashtags'] = df.entities[i]['hashtags']\n",
    "            dict_tmp['text'] = df.text[i]\n",
    "        dict_tot[(i,-1)]=dict_tmp\n",
    "    elif type(df.text[i])==pd.core.series.Series: \n",
    "        for j in range(len(df.text[i])): \n",
    "            dict_tmp = dict()\n",
    "            if (type(df.text[i].reset_index().text[j])==str):\n",
    "                if len(df.entities[i].reset_index().entities[j]['hashtags'])!=0:\n",
    "                    dict_tmp['hashtags'] = df.entities[i].reset_index().entities[j]['hashtags'][0]['text']\n",
    "                    dict_tmp['text'] = df.text[i].reset_index()['text'][j]\n",
    "                else : \n",
    "                    dict_tmp['hashtags'] = df.entities[i].reset_index().entities[j]['hashtags']\n",
    "                    dict_tmp['text'] = df.text[i].reset_index()['text'][j]\n",
    "                dict_tot[(i,j)]=dict_tmp\n",
    "pickle.dump(dict_tot, open('dictionary_with_all_tweets_and_hashtags.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tot = pickle.load(open('dictionary_with_all_tweets_and_hashtags.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the list of investor and compagny we are working on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "compagny = pickle.load(open('compagny.p', 'rb'))\n",
    "investor = pickle.load(open('investor.p', 'rb'))\n",
    "############# ATTENTION IF FAUT QUE JE CLEAN CES NOMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e9a90f14cbdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcompagny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompagny\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minvestor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvestor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdict_tot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ALTABA INC is a really nice compagny'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdict_tot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'I am never disappointed with ALTABA INC'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdict_tot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ALTABA INC have nice product'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0, 0)"
     ]
    }
   ],
   "source": [
    "# Hard code data and try on a small subset\n",
    "compagny = compagny[0:5]\n",
    "investor = investor[0:5]\n",
    "dict_tot[(0,0)]['text'] = 'ALTABA INC is a really nice compagny'\n",
    "dict_tot[(0,1)]['text'] = 'I am never disappointed with ALTABA INC'\n",
    "dict_tot[(0,2)]['text'] = 'ALTABA INC have nice product'\n",
    "dict_tot[(0,3)]['text'] = 'ALTABA INC <3 :)'\n",
    "dict_tot[(0,4)]['text'] = 'good bro !! ALTABA INC '\n",
    "dict_tot[(0,5)]['text'] = 'CATERPILLAR INC DEL will kill the world'\n",
    "dict_tot[(0,6)]['text'] = 'CATERPILLAR INC DEL is really bad'\n",
    "dict_tot[(0,7)]['text'] = 'I am really angry against CATERPILLAR INC DEL '\n",
    "dict_tot[(0,8)]['text'] = 'Shit !!! CATERPILLAR INC DEL '\n",
    "dict_tot[(0,9)]['text'] = 'CATERPILLAR INC DEL :( :( :('\n",
    "dict_tot[(0,10)]['text'] = 'SPDR S&P 500 ETF TR sell some vegetables'\n",
    "dict_tot[(0,11)]['text'] = 'SPDR S&P 500 ETF TR play football'\n",
    "dict_tot[(0,12)]['text'] = 'SPDR S&P 500 ETF TR is located in lausanne'\n",
    "dict_tot[(0,13)]['text'] = 'SPDR S&P 500 ETF TR is in another country'\n",
    "dict_tot[(0,14)]['text'] = '__ __ SPDR S&P 500 ETF TR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary using the compagny or investor as key grouping all the tweets about it\n",
    "We consider that the tweet concerned a compagny/investor is there is his name in the tweet or the hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_per_category(list_of_names, dict_tot):\n",
    "    dict_per_category = dict()\n",
    "\n",
    "    for name in list_of_names :\n",
    "        list_tweet = []\n",
    "        for key in list(dict_tot.keys()) :\n",
    "            if (str(dict_tot[key]['hashtags']).lower().find(name.lower()) != -1) | (dict_tot[key]['text'].lower().find(name.lower()) != -1):\n",
    "                list_tweet.append(dict_tot[key]['text'])\n",
    "        dict_per_category[name] = list_tweet\n",
    "    return dict_per_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_per_compagny = tweet_per_category(compagny, dict_tot)\n",
    "pickle.dump(dict_per_compagny, open('dictionary_per_compagny_tweet.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ALTABA INC': ['ALTABA INC is a really nice compagny',\n",
       "  'I am never disappointed with ALTABA INC',\n",
       "  'ALTABA INC have nice product',\n",
       "  'ALTABA INC <3 :)',\n",
       "  'good bro !! ALTABA INC '],\n",
       " 'SPDR S&P 500 ETF TR': [],\n",
       " 'CATERPILLAR INC DEL': ['CATERPILLAR INC DEL will kill the world',\n",
       "  'CATERPILLAR INC DEL is really bad',\n",
       "  'I am really angry against CATERPILLAR INC DEL ',\n",
       "  'Shit !!! CATERPILLAR INC DEL ',\n",
       "  'CATERPILLAR INC DEL :( :( :('],\n",
       " 'DOWDUPONT INC': [],\n",
       " 'INVESCO QQQ TR': []}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_per_compagny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_per_investor = tweet_per_category(investor, dict_tot)\n",
    "pickle.dump(dict_per_investor, open('dictionary_per_investor_tweet.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KINGDON CAPITAL MANAGEMENT, L.L.C.': [],\n",
       " 'ROYAL BANK OF CANADA': [],\n",
       " 'GIRARD PARTNERS LTD.': [],\n",
       " 'SANDLER CAPITAL MANAGEMENT': [],\n",
       " 'BROOKFIELD ASSET MANAGEMENT INC.': []}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_per_investor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tot = pickle.load(open('dictionary_per_compagny_tweet.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tot = pickle.load(open('dictionary_per_investor_tweet.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sentiment analysis for each tweet for each compagny/investors :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vader\n",
    "This library takes into account :\n",
    "- positive sentence example\n",
    "- punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "- booster words handled correctly (sentiment intensity adjusted)\n",
    "- emphasis for ALLCAPS handled\n",
    "- combination of signals - VADER appropriately adjusts intensity\n",
    "-  booster words & punctuation make this close to ceiling for score\n",
    "- negation sentence example\n",
    "- positive sentence\n",
    "- negated negative sentence with contraction\n",
    "- qualified positive sentence is handled correctly (intensity adjusted)\n",
    "- mixed negation sentence\n",
    "- negative slang with capitalization emphasis\n",
    "- mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "- emoticons handled\n",
    "- emojis handled\n",
    "- Capitalized negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_analysis(dict_per_):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    dict_score = dict()\n",
    "    for key in list(dict_per_.keys()):\n",
    "        neg, pos, neu, compound, tmp_dict = [], [], [], [], dict()\n",
    "        for sentence in dict_per_[key]:\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            neg.append(vs['neg'])\n",
    "            neu.append(vs['neu'])\n",
    "            pos.append(vs['pos'])\n",
    "            compound.append(vs['compound'])\n",
    "        tmp_dict['neg'] = np.mean(neg)\n",
    "        tmp_dict['pos'] = np.mean(pos)\n",
    "        tmp_dict['neu'] = np.mean(neu)\n",
    "        tmp_dict['compound'] = np.mean(compound)\n",
    "        dict_score[key] = tmp_dict \n",
    "        \n",
    "    return dict_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ALTABA INC': {'neg': 0.0,\n",
       "  'pos': 0.46900000000000003,\n",
       "  'neu': 0.531,\n",
       "  'compound': 0.50376},\n",
       " 'SPDR S&P 500 ETF TR': {'neg': nan, 'pos': nan, 'neu': nan, 'compound': nan},\n",
       " 'CATERPILLAR INC DEL': {'neg': 0.5034000000000001,\n",
       "  'pos': 0.0,\n",
       "  'neu': 0.49659999999999993,\n",
       "  'compound': -0.6654},\n",
       " 'DOWDUPONT INC': {'neg': nan, 'pos': nan, 'neu': nan, 'compound': nan},\n",
       " 'INVESCO QQQ TR': {'neg': nan, 'pos': nan, 'neu': nan, 'compound': nan}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = vader_analysis(dict_per_compagny)\n",
    "pickle.dump(output, open('dictionary_per_compagny_score_vader.p', 'wb'))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KINGDON CAPITAL MANAGEMENT, L.L.C.': {'neg': nan,\n",
       "  'pos': nan,\n",
       "  'neu': nan,\n",
       "  'compound': nan},\n",
       " 'ROYAL BANK OF CANADA': {'neg': nan, 'pos': nan, 'neu': nan, 'compound': nan},\n",
       " 'GIRARD PARTNERS LTD.': {'neg': nan, 'pos': nan, 'neu': nan, 'compound': nan},\n",
       " 'SANDLER CAPITAL MANAGEMENT': {'neg': nan,\n",
       "  'pos': nan,\n",
       "  'neu': nan,\n",
       "  'compound': nan},\n",
       " 'BROOKFIELD ASSET MANAGEMENT INC.': {'neg': nan,\n",
       "  'pos': nan,\n",
       "  'neu': nan,\n",
       "  'compound': nan}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = vader_analysis(dict_per_investor)\n",
    "pickle.dump(output, open('dictionary_per_investor_score_vader.p', 'wb'))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "import re\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR_PATH = \"../../Downloads/\"\n",
    "\n",
    "model = fastText.load_model(os.path.join(MODEL_DIR_PATH, \"amazon_review_full.bin\"))\n",
    "\n",
    "maketrans = str.maketrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing to clean text data.\n",
    "    \n",
    "    In particular:\n",
    "    - lowers the string\n",
    "    - removes the character [']\n",
    "    - replaces punctuation characters with spaces\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"\\'\", \"\", text)  # remove the character [']\n",
    "\n",
    "    # removing the punctuation\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    split = \" \"\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        translate_map = dict((ord(c), str(split)) for c in filters)\n",
    "        text = text.translate(translate_map)\n",
    "    elif len(split) == 1:\n",
    "        translate_map = maketrans(filters, split * len(filters))\n",
    "        text = text.translate(translate_map)\n",
    "    else:\n",
    "        for c in filters:\n",
    "            text = text.replace(c, split)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment = lambda s: model.predict(clean_text(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastext_prediction(dict_per_):\n",
    "    dict_score = dict()\n",
    "    for key in list(dict_per_.keys()):\n",
    "        label, confidence, tmp_dict = [], [], dict()\n",
    "        for sentence in dict_per_[key]:\n",
    "            res = predict_sentiment(sentence)\n",
    "            label.append(int(res[0][0][9]))\n",
    "            confidence.append(res[1][0])\n",
    "        tmp_dict['label'] = np.mean(label)\n",
    "        tmp_dict['confidence'] = np.mean(confidence)\n",
    "        dict_score[key] = tmp_dict \n",
    "\n",
    "    return dict_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ALTABA INC': {'label': 3.0, 'confidence': 0.8472848296165466},\n",
       " 'SPDR S&P 500 ETF TR': {'label': nan, 'confidence': nan},\n",
       " 'CATERPILLAR INC DEL': {'label': 1.8, 'confidence': 0.7830009698867798},\n",
       " 'DOWDUPONT INC': {'label': nan, 'confidence': nan},\n",
       " 'INVESCO QQQ TR': {'label': nan, 'confidence': nan}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = fastext_prediction(dict_per_compagny)\n",
    "pickle.dump(output, open('dictionary_per_compagny_score_fastext.p', 'wb'))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KINGDON CAPITAL MANAGEMENT, L.L.C.': {'label': nan, 'confidence': nan},\n",
       " 'ROYAL BANK OF CANADA': {'label': nan, 'confidence': nan},\n",
       " 'GIRARD PARTNERS LTD.': {'label': nan, 'confidence': nan},\n",
       " 'SANDLER CAPITAL MANAGEMENT': {'label': nan, 'confidence': nan},\n",
       " 'BROOKFIELD ASSET MANAGEMENT INC.': {'label': nan, 'confidence': nan}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = fastext_prediction(dict_per_investor)\n",
    "pickle.dump(output, open('dictionary_per_investor_score_fastext.p', 'wb'))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
