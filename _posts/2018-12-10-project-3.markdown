---
title: Sentiment Analysis
subtitle: Based on Tweets from 2017
layout: default
modal-id: 3
date: 2018-12-10
img: twitter.jpg
thumbnail: twitter.png
alt: image-alt
project-date: December 2018
client: EPFL - ADA Course
category: Sentiments
description: During this section, you will get insight on how it was possible to link tweets to sentiments and opinions about companies through sentiment analysis. 

---

## Introduction

Public opinion is a important criteria to choose a investement. In fact, each investement will be a part of the identity of the inve stor and public opinion could really sustain some investement and criticize others.
It is thus interesting to correlate the greater public's interest into social good projects expressed on social media platforms such as Twitter and Facebook, or news platforms, with the actual investments that are done. 
To add the public opinion part, sentiment analysis was done on tweet from 2017. In fact, sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. In this part of the data story, the text classifier process tool was used to analyses an incoming message and tells whether the underlying sentiment is positive, negative our neutral.

**Used Dataset:** The raw dataset was composed of all the tweets from 2017. 
First of all, output of the last part where taken and mix with a new dataset which is the Twitter dataset given in the course. It contains the tweets of 2017 and it’s store on the ADA cluster. The documentation of Twitter API was analyzed to understand how to use this database. The group decided to work on a selected part of the entire dataset. In fact, our goal is to score the public’s opinion of compagnies thanks to a sentiment analysis done over tweets of 2017. To do that we decide to take just tweet corresponding to 3 days per month. English tweets were filter as our analysis is made on Americans compagnies. We thus care more about the public opinion of English speaking population as they are more impacted by theses compagnies. For each month, we take always the same days : the 1th, the 11th one and the 21th. Then a preprocessing part corresponding to the cleaning of the tweets and the creation of dictionaries was done. Moreover, we link each tweets to compagny. To do that a easy rule was used : a tweet concerned a company if the name of the company is present in the tweet text or in the tweet tags.


## Research Question 
How to make a ranking of “public opinion” regarding a company?
What is the great public's opinion regarding the companies and investors ? 
Just capital ranking is one thing. This metrics is complex and tries to take into account a high number of variables. But it just capital score correlated with public opinion ? It means that we will try to understand the public’s opinion regarding the companies that held many investments in social good versus those who do not invest in social good?

## Results 

<iframe width="900" height="800" frameborder="0" scrolling="no" src="//plot.ly/~lesimplen/1.embed"></iframe>


## Discussion 

Unfortunatly, in this sentiment analysis, some bias can be highlighted.
First of all, with the twitter dataset consisted of the tweet from 2017 we wanted to gives a indication of the public opinion per compagnies/investors. Therefore, we did not need to take the data from everyday. It was decided that it would be wiser and sufficient to take 10-days samples through the year 2017. This made the calculation feasible in Pandas. Since we were looking for information about investors and companies in the USA, we decided to only look at the tweet in English. This reduces the number of tweet per day by roughly 70% the majority of the Tweet are written in the mother tongue of the user (Chinese, Japanese, German, ...). All theses assumptions can create bias.
Moreover, to link tweets and and compagnies/investors names, we search in tweets text and tags the name of the corresponding actors. This method makes senses but as language is a really complex process with a lot of possibilities, some tweets corresponding to one categorical compagny/investor could have not being linked.
To conlude this discussion, fastText method gives labels from 1 (negative sentence) to 5 (positive sentence) which is not a score between -1 and 1. Thus, we decide to add this labels number as integer and do a mean. In fact we choose the mean to penalized well extreme values. This way of creating a score can introduce a bias as the method is not made for that. Scores for actors with one tweet would be less robust than with several tweets.

## Conclusion

By having all theses potential bias in mind, we could draw conclusion from the analysis of the results.

A COMPLETER
